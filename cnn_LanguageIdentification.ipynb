{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sample Filename     Language\n",
      "0  000kouqjfnk.mp3    Hmong Daw\n",
      "1  000w3fewuqj.mp3    Tektiteko\n",
      "2  000ylhu4sxl.mp3       Teribe\n",
      "3  0014x3zvjrl.mp3      Chipaya\n",
      "4  001xjmtk2wx.mp3  KalmykOirat\n",
      "Total languages:  176\n",
      "Keras version:  2.1.4\n"
     ]
    }
   ],
   "source": [
    "#Create directory structure\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tables\n",
    "from sklearn.datasets import load_files       \n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "trainingdata_directory = '/home/ubuntu/src/capstone/datafiles/trainingdata/'\n",
    "trainingdata_spectrograms = '/home/ubuntu/src/capstone/datafiles/trainingdata/training_spectrograms_complete'\n",
    "os.chdir(trainingdata_directory)\n",
    "data = pd.read_csv('trainingData.csv')\n",
    "print(data.head())\n",
    "\n",
    "languages = np.unique(data['Language'])\n",
    "print('Total languages: ', len(languages))\n",
    "\n",
    "print('Keras version: ', keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create train and valid data sets, arrange input spectrograms to suit Keras image generator.\n",
    "#1. Create directories train and valid.\n",
    "#2. Create 176 directories with language labels inside train and valid directories.\n",
    "\n",
    "#Code is commented out as this step is required only once.\n",
    "\n",
    "#Create directories with 176 language names in train and valid directories.\n",
    "'''\n",
    "if os.path.exists(trainingdata_spectrograms):\n",
    "    os.chdir(trainingdata_spectrograms)\n",
    "    if not os.path.exists('valid'):\n",
    "        os.makedirs('valid')\n",
    "    if os.path.exists('valid'):\n",
    "        os.chdir('valid')\n",
    "        for language in languages:\n",
    "            if not os.path.isdir(trainingdata_spectrograms + '/' + 'valid' + '/' + language):\n",
    "                os.makedirs(trainingdata_spectrograms + '/' + 'valid' + '/' + language)\n",
    "    os.chdir(trainingdata_spectrograms)\n",
    "    if not os.path.exists('train'):\n",
    "        os.makedirs('train')\n",
    "    if os.path.exists('train'):\n",
    "        os.chdir('train')\n",
    "        for language in languages:\n",
    "            if not os.path.isdir(trainingdata_spectrograms + '/' + 'train' + '/' + language):\n",
    "                os.makedirs(trainingdata_spectrograms + '/' + 'train' + '/' + language)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trainingData.csv is read in the previous step which matches training files to correct language label.\n",
    "#    Move roughly 20% images for validation remaining images in train directory.\n",
    "'''\n",
    "#transfer train and valid data files.\n",
    "index = 0\n",
    "if os.path.exists(trainingdata_spectrograms):\n",
    "    os.chdir(trainingdata_spectrograms)\n",
    "    \n",
    "    for language in languages:\n",
    "        os.chdir(trainingdata_spectrograms)\n",
    "        index = 0\n",
    "        if os.path.isdir(language): \n",
    "            os.chdir(language)\n",
    "            for file in os.listdir('.'):\n",
    "                if os.path.isfile(file):\n",
    "                    #print(index)\n",
    "                    if index < 75:\n",
    "                        destination_path = trainingdata_spectrograms + '/' +  'valid' + '/' + language + '/'  + file\n",
    "                    else:\n",
    "                        destination_path = trainingdata_spectrograms + '/' +  'train' + '/' + language + '/'  + file\n",
    "                    source_path = file\n",
    "                    os.rename(source_path, destination_path)\n",
    "                    index += 1\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 176 total language categories.\n",
      "There are 52625 total train spectrgrams.\n",
      "\n",
      "There are 13551 total valid spectrgrams.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Taken from Udacity dog project\n",
    "#Used to fit mode without using Keras image generator. \n",
    "#Not useful when using  image generator as Keras image generator flow_from_directory method reads input images automatically.\n",
    "\n",
    "os.chdir(trainingdata_directory)\n",
    "\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    language_files = np.array(data['filenames'])\n",
    "    language_targets = np_utils.to_categorical(np.array(data['target']), 176)\n",
    "    return language_files, language_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset(trainingdata_spectrograms + '/' + 'train')\n",
    "valid_files, valid_targets = load_dataset(trainingdata_spectrograms + '/' + 'valid')\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total language categories.' % len(languages))\n",
    "print('There are %s total train spectrgrams.\\n' % len(train_files))\n",
    "print('There are %s total valid spectrgrams.\\n' % len(valid_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#From Udacity dog project. Used to create 4D tensors for Keras.\n",
    "#This step is not required when using Keras image generator.\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(200, 100))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (200, 100, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 200, 100, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors1 = paths_to_tensor(train_files[:len(train_files)//2]).astype('float32')/255\n",
    "train_tensors2 = paths_to_tensor(train_files[len(train_files)//2:]).astype('float32')/255\n",
    "#valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "#test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import pickles #Causes memory overflow error (max 4GB allowed, use hdf5)\n",
    "# Store the 4D tensors on disk. This is doen to avoid calculating 4D tensors each time this jupyter notebook is run\n",
    "#which is time consuming.\n",
    "#This step is not required when using Keras image generator.\n",
    "\n",
    "if not os.path.isfile('train_tensors1.h5'):\n",
    "    h5file = tables.open_file('train_tensors1.h5', mode='w', title=\"Test Array\")\n",
    "    root = h5file.root\n",
    "    h5file.create_array(root, \"train_tensors1\", train_tensors1)\n",
    "    h5file.close()\n",
    "\n",
    "if not os.path.isfile('train_tensors2.h5'):\n",
    "    h5file = tables.open_file('train_tensors2.h5', mode='w', title=\"Test Array\")\n",
    "    root = h5file.root\n",
    "    h5file.create_array(root, \"train_tensors2\", train_tensors2)\n",
    "    h5file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read pre-computed tesnors, saves time to generate again.\n",
    "#This step is not required when using Keras image generator.\n",
    "\n",
    "h5file = tables.open_file('train_tensors1.h5', mode='r')\n",
    "root = h5file.root\n",
    "train_tensors_1 = np.array(np.zeros(66176//2))\n",
    "for array in h5file.walk_nodes(root , 'Array'):\n",
    "    train_tensors_1 = np.array(array)\n",
    "    print('array.shape: ', train_tensors_1.shape)\n",
    "h5file.close()\n",
    "\n",
    "train_tensors_2 = np.array(np.zeros(66176//2))\n",
    "h5file = tables.open_file('train_tensors2.h5', mode='r')\n",
    "root = h5file.root\n",
    "for array in h5file.walk_nodes(root , 'Array'):\n",
    "    train_tensors_2 = np.array(array)\n",
    "    print('array.shape: ', train_tensors_2.shape)\n",
    "h5file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 200, 100, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 100, 50, 16)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100, 50, 16)       64        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 100, 50, 32)       4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 50, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50, 25, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 25, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 25, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 25, 12, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 25, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 6, 128)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 6, 128)        512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 6, 128)        147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 6, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 6, 3, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 3, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 3, 1, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              787456    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 176)               180400    \n",
      "=================================================================\n",
      "Total params: 1,510,544\n",
      "Trainable params: 1,509,296\n",
      "Non-trainable params: 1,248\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define CNN model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "### TODO: Define your architecture.\n",
    "\n",
    "# 1 Conv Relu BPool \n",
    "model.add(Conv2D(filters=16, kernel_size=(3,3), padding='same', activation='relu', \n",
    "                        input_shape=(200, 100, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 2 Conv Relu BPool \n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 3 Conv Relu BPool \n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 4 Conv Relu BPool \n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 5 Conv Relu BPool \n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# 6 Conv Relu BPool \n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "#model.add(GlobalAveragePooling2D())\n",
    "\n",
    "# 1 Dense\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "#model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "#2 Dense final layer\n",
    "model.add(Dense(176, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load weights from previous run\n",
    "#model.load_weights(filepath='cnn_try1_2.h5')\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 30\n",
    "learning_rate = 0.001\n",
    "decay_rate = 1e-6\n",
    "#decay_rate = learning_rate / epochs\n",
    "momentum = 0.8\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "model.load_weights('cnn_ImageGenerator_2.ipynb.h5')\n",
    "#model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "#print(train_tensors_1.shape)\n",
    "#print(train_tensors_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52625 images belonging to 176 classes.\n",
      "Found 13551 images belonging to 176 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=0, save_best_only=True)\n",
    "\n",
    "train_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        trainingdata_spectrograms + '/' + 'train',\n",
    "        target_size=(200, 100),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle = False)\n",
    "\n",
    "test_datagen = ImageDataGenerator()\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        trainingdata_spectrograms + '/' + 'valid',\n",
    "        target_size=(200, 100),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1754/1754 [==============================] - 121s 69ms/step - loss: 0.0268 - acc: 0.9975 - val_loss: 3.1816 - val_acc: 0.2913\n",
      "Epoch 2/20\n",
      "1754/1754 [==============================] - 120s 68ms/step - loss: 0.0239 - acc: 0.9981 - val_loss: 3.2096 - val_acc: 0.2924\n",
      "Epoch 3/20\n",
      "1754/1754 [==============================] - 119s 68ms/step - loss: 0.0213 - acc: 0.9985 - val_loss: 3.1346 - val_acc: 0.3008\n",
      "Epoch 4/20\n",
      "1754/1754 [==============================] - 119s 68ms/step - loss: 0.0185 - acc: 0.9991 - val_loss: 2.9604 - val_acc: 0.3288\n",
      "Epoch 5/20\n",
      "1754/1754 [==============================] - 119s 68ms/step - loss: 0.0170 - acc: 0.9990 - val_loss: 2.8810 - val_acc: 0.3346\n",
      "Epoch 6/20\n",
      "1754/1754 [==============================] - 119s 68ms/step - loss: 0.0155 - acc: 0.9993 - val_loss: 2.9379 - val_acc: 0.3286\n",
      "Epoch 7/20\n",
      "1754/1754 [==============================] - 120s 68ms/step - loss: 0.0141 - acc: 0.9995 - val_loss: 3.0551 - val_acc: 0.3177\n",
      "Epoch 8/20\n",
      "1754/1754 [==============================] - 119s 68ms/step - loss: 0.0135 - acc: 0.9993 - val_loss: 2.9752 - val_acc: 0.3228\n",
      "Epoch 9/20\n",
      "1754/1754 [==============================] - 119s 68ms/step - loss: 0.0123 - acc: 0.9994 - val_loss: 3.1841 - val_acc: 0.3091\n",
      "Epoch 10/20\n",
      "1754/1754 [==============================] - 119s 68ms/step - loss: 0.0116 - acc: 0.9996 - val_loss: 3.2057 - val_acc: 0.3023\n",
      "Epoch 11/20\n",
      "1754/1754 [==============================] - 119s 68ms/step - loss: 0.0109 - acc: 0.9995 - val_loss: 3.0080 - val_acc: 0.3228\n",
      "Epoch 12/20\n",
      "1754/1754 [==============================] - 119s 68ms/step - loss: 0.0099 - acc: 0.9997 - val_loss: 3.1056 - val_acc: 0.3149\n",
      "Epoch 13/20\n",
      "1754/1754 [==============================] - 119s 68ms/step - loss: 0.0095 - acc: 0.9997 - val_loss: 3.1290 - val_acc: 0.3040\n",
      "Epoch 14/20\n",
      "1754/1754 [==============================] - 119s 68ms/step - loss: 0.0089 - acc: 0.9997 - val_loss: 3.0051 - val_acc: 0.3258\n",
      "Epoch 15/20\n",
      "1754/1754 [==============================] - 119s 68ms/step - loss: 0.0087 - acc: 0.9998 - val_loss: 3.0949 - val_acc: 0.3151\n",
      "Epoch 16/20\n",
      "1754/1754 [==============================] - 119s 68ms/step - loss: 0.0081 - acc: 0.9998 - val_loss: 3.1158 - val_acc: 0.3086\n",
      "Epoch 17/20\n",
      "1754/1754 [==============================] - 119s 68ms/step - loss: 0.0079 - acc: 0.9996 - val_loss: 3.0404 - val_acc: 0.3174\n",
      "Epoch 18/20\n",
      "1754/1754 [==============================] - 119s 68ms/step - loss: 0.0072 - acc: 0.9999 - val_loss: 3.1078 - val_acc: 0.3122\n",
      "Epoch 19/20\n",
      "1754/1754 [==============================] - 119s 68ms/step - loss: 0.0071 - acc: 0.9998 - val_loss: 3.2108 - val_acc: 0.3058\n",
      "Epoch 20/20\n",
      "1754/1754 [==============================] - 119s 68ms/step - loss: 0.0069 - acc: 0.9998 - val_loss: 3.1932 - val_acc: 0.3129\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=52625 // batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=13551 // batch_size)\n",
    "\n",
    "\n",
    "#model.fit(np.vstack([train_tensors_1, train_tensors_1]), train_targets, \n",
    "#          validation_split = 0.20,\n",
    "#          epochs=epochs, batch_size=30, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "model.save('cnn_ImageGenerator_2.ipynb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13551 images belonging to 176 classes.\n",
      "452/452 [==============================] - 99s 219ms/step\n",
      "len(bottleneck_features_train1) 13551\n"
     ]
    }
   ],
   "source": [
    "#Create test tensors and check results.\n",
    "\n",
    "testingdata_directory = '/home/ubuntu/src/capstone/datafiles/testingdata/'\n",
    "testingdata_spectrograms = '/home/ubuntu/src/capstone/datafiles/testingdata/testing_spectrograms_complete'\n",
    "\n",
    "#data = load_files(testingdata_spectrograms )\n",
    "#print('type(data): ', type(data))\n",
    "#test_files = np.array(data['filenames'])\n",
    "#test_tensors =  paths_to_tensor(test_files).astype('float32')/255\n",
    "#test_tensors2 =  paths_to_tensor(test_files[6159:]).astype('float32')/255\n",
    "# print statistics about the dataset\n",
    "#print('There are %s total test spectrgrams.\\n' % len(test_tensors))\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "        trainingdata_spectrograms + '/' + 'valid',\n",
    "        target_size=(200, 100),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',  # this means our generator will only yield batches of data, no labels\n",
    "        shuffle=False)\n",
    "# the predict_generator method returns the output of a model, given\n",
    "# a generator that yields batches of numpy data\n",
    "bottleneck_features_train = model.predict_generator(test_generator, verbose=1)\n",
    "#bottleneck_features_train = [model.predict(np.expand_dims(test_tensor, axis=0), verbose=0) for test_tensor in test_tensors]\n",
    "#test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "print('len(bottleneck_features_train1)', len(bottleneck_features_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_target_prediction = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "#print('len(test_target_prediction)', len(test_target_prediction))\n",
    "#test_target_prediction = model.predict(test_tensors, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(bottleneck_features_train[100]))\n",
    "print(np.argmax(bottleneck_features_train[0]))\n",
    "language_ids = [np.argmax(language_prediction) for language_prediction in bottleneck_features_train]\n",
    "#print(len(language_ids))\n",
    "#Take list of test image files used by the generator during prediction.\n",
    "test_gen_files = test_generator.filenames\n",
    "#change file extensions to mp3.\n",
    "test_files = []\n",
    "for file in test_gen_files:\n",
    "    filename, extension = os.path.splitext(file)\n",
    "    filename = filename[5:] + '.mp3'\n",
    "    test_files.append(filename)\n",
    "\n",
    "class_index = pd.DataFrame(list(train_generator.class_indices.items()))\n",
    "\n",
    "test_targets =[]\n",
    "for id in language_ids:\n",
    "    test_targets.append(class_index[0][id])\n",
    "\n",
    "#create test_file, test_target dataFrame\n",
    "test_output = pd.DataFrame(test_targets, test_files)\n",
    "test_output.to_csv('test_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gprzb5txc5o', '4096-128', 'png']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file = 'gprzb5txc5o.4096-128.png'\n",
    "\n",
    "os.path.basename(file).split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
